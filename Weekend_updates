{"cells":[{"cell_type":"markdown","metadata":{"id":"_0a-kMxEfjsC"},"source":["Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6150,"status":"ok","timestamp":1687875712427,"user":{"displayName":"Alessandro Belotti","userId":"09646628518605735287"},"user_tz":-120},"id":"BdiZVOMi20IV","outputId":"301cb458-e6de-4dd5-bdc9-c0e63b42be09"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement locale (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for locale\u001b[0m\u001b[31m\n","\u001b[0mGenerating locales (this might take a while)...\n","  en_US.UTF-8... done\n","Generation complete.\n"]}],"source":["!pip install -q -U locale\n","!locale-gen en_US.UTF-8"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27783,"status":"ok","timestamp":1687875740206,"user":{"displayName":"Alessandro Belotti","userId":"09646628518605735287"},"user_tz":-120},"id":"N4O78d_tJO4W","outputId":"8c677587-c8d0-4dae-b1b5-a8e6d549fd90"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting emoji\n","  Downloading emoji-2.5.1.tar.gz (356 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.3/356.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-2.5.1-py2.py3-none-any.whl size=351210 sha256=f7567068fa87061944b4b8db9692b902ec3239f04c6e08cc52f4242aa96e9538\n","  Stored in directory: /root/.cache/pip/wheels/51/92/44/e2ef13f803aa08711819357e6de0c5fe67b874671141413565\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-2.5.1\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package names to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/names.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package state_union to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/state_union.zip.\n","[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/twitter_samples.zip.\n","[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/movie_reviews.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","os.environ['LC_ALL'] = 'en_US.UTF-8'\n","os.environ['LANG'] = 'en_US.UTF-8'\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","from google.colab import drive\n","from shutil import copyfile\n","import zipfile\n","from matplotlib import pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import TextVectorization\n","from keras.layers import Embedding\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","from pprint import pprint\n","import string\n","from collections import Counter\n","\n","!pip install emoji\n","import emoji\n","import torch\n","\n","import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","nltk.download([\n","    \"wordnet\",\n","    \"omw-1.4\",\n","    \"punkt\",\n","    \"names\",\n","    \"stopwords\",\n","    \"state_union\",\n","    \"twitter_samples\",\n","    \"movie_reviews\",\n","    \"averaged_perceptron_tagger\",\n","    \"vader_lexicon\",\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"a1dsQN2WCADv","outputId":"1d54379a-3e0d-4a22-ba4a-1863c6889036"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting textattack\n","  Downloading textattack-0.3.8-py3-none-any.whl (418 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.7/418.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bert-score>=0.3.5 (from textattack)\n","  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack) (0.6.2)\n","Collecting flair (from textattack)\n","  Downloading flair-0.12.2-py3-none-any.whl (373 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.1/373.1 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack) (3.12.2)\n","Collecting language-tool-python (from textattack)\n","  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n","Collecting lemminflect (from textattack)\n","  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting lru-dict (from textattack)\n","  Downloading lru_dict-1.2.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n","Collecting datasets==2.4.0 (from textattack)\n","  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack) (3.8.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.22.4)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.5.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.10.1)\n","Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.0.1+cu118)\n","Collecting transformers>=4.21.0 (from textattack)\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting terminaltables (from textattack)\n","  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack) (4.65.0)\n","Collecting word2number (from textattack)\n","  Downloading word2number-1.1.zip (9.7 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting num2words (from textattack)\n","  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack) (9.1.0)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.7.1)\n","Collecting pinyin==0.4.0 (from textattack)\n","  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack) (0.42.1)\n","Collecting OpenHowNet (from textattack)\n","  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n","Collecting pycld2 (from textattack)\n","  Downloading pycld2-0.41.tar.gz (41.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting click<8.1.0 (from textattack)\n","  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (9.0.0)\n","Collecting dill<0.3.6 (from datasets==2.4.0->textattack)\n","  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (2.27.1)\n","Collecting xxhash (from datasets==2.4.0->textattack)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m838.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets==2.4.0->textattack)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (3.8.4)\n","Collecting huggingface-hub<1.0.0,>=0.1.0 (from datasets==2.4.0->textattack)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.4.0->textattack) (23.1)\n","Collecting responses<0.19 (from datasets==2.4.0->textattack)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (3.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2022.7.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.6.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.8,>=1.7.0->textattack) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.8,>=1.7.0->textattack) (16.0.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.21.0->textattack) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.21.0->textattack) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.21.0->textattack)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.21.0->textattack)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gensim>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.3.1)\n","Collecting segtok>=1.5.7 (from flair->textattack)\n","  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n","Collecting mpld3==0.3 (from flair->textattack)\n","  Downloading mpld3-0.3.tar.gz (788 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.2.2)\n","Collecting sqlitedict>=1.6.0 (from flair->textattack)\n","  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting deprecated>=1.2.4 (from flair->textattack)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: hyperopt>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.2.7)\n","Collecting boto3 (from flair->textattack)\n","  Downloading boto3-1.26.161-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bpemb>=0.3.2 (from flair->textattack)\n","  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.8.10)\n","Collecting langdetect (from flair->textattack)\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.9.2)\n","Collecting ftfy (from flair->textattack)\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting janome (from flair->textattack)\n","  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gdown==4.4.0 (from flair->textattack)\n","  Downloading gdown-4.4.0.tar.gz (14 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting conllu>=4.0 (from flair->textattack)\n","  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n","Collecting wikipedia-api (from flair->textattack)\n","  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n","Collecting pptree (from flair->textattack)\n","  Downloading pptree-3.1.tar.gz (3.0 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pytorch-revgrad (from flair->textattack)\n","  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n","Collecting transformer-smaller-training-vocab>=0.2.1 (from flair->textattack)\n","  Downloading transformer_smaller_training_vocab-0.2.4-py3-none-any.whl (13 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown==4.4.0->flair->textattack) (1.16.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.4.0->flair->textattack) (4.11.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack) (1.2.0)\n","Collecting docopt>=0.6.2 (from num2words->textattack)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting anytree (from OpenHowNet->textattack)\n","  Downloading anytree-2.9.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (67.7.2)\n","Collecting sentencepiece (from bpemb>=0.3.2->flair->textattack)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.4->flair->textattack) (1.14.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.4.0->textattack) (1.3.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=3.8.0->flair->textattack) (6.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack) (0.18.3)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack) (2.2.1)\n","Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack) (0.10.9.7)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.40.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.4)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.1.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (2023.5.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.4.0->textattack) (3.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->flair->textattack) (3.1.0)\n","Requirement already satisfied: protobuf<=3.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.21.0->textattack) (3.20.3)\n","Collecting botocore<1.30.0,>=1.29.161 (from boto3->flair->textattack)\n","  Downloading botocore-1.29.161-py3-none-any.whl (10.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->flair->textattack)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->flair->textattack)\n","  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->flair->textattack) (0.2.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.3)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","Collecting multiprocess (from datasets==2.4.0->textattack)\n","  Downloading multiprocess-0.70.13-py310-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n","Collecting accelerate>=0.20.2 (from transformers>=4.21.0->textattack)\n","  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.4.0->flair->textattack) (2.4.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.2->transformers>=4.21.0->textattack) (5.9.5)\n","Building wheels for collected packages: pinyin, gdown, mpld3, pycld2, word2number, docopt, sqlitedict, langdetect, pptree\n","  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630476 sha256=e0f762625f1902aa85224acb2a9f907af3103785711009f2cad3e24740a68df8\n","  Stored in directory: /root/.cache/pip/wheels/33/38/af/616fc6f154aa5bae65a1da12b22d79943434269f0468ff9b3f\n","  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14759 sha256=343c0c8dd831103f02f8f51cecdba3b6e680c593265267c5e431f1f850023fbe\n","  Stored in directory: /root/.cache/pip/wheels/03/0b/3f/6ddf67a417a5b400b213b0bb772a50276c199a386b12c06bfc\n","  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116685 sha256=38d95a42348d2da90871a50818208043503de0d1ca1caed399033955e5d4a02a\n","  Stored in directory: /root/.cache/pip/wheels/9c/92/f7/45d9aac5dcfb1c2a1761a272365599cc7ba1050ce211a3fd9a\n","  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycld2: filename=pycld2-0.41-cp310-cp310-linux_x86_64.whl size=9915869 sha256=ea7988b1733d7f95039414932d4125d733d7fe4fe70a3a38127cef51a04dc768\n","  Stored in directory: /root/.cache/pip/wheels/be/81/31/240c89c845e008a93d98542325270007de595bfd356eb0b06c\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5569 sha256=5b84ce3c04633ad9725b7035734701b89390d6f690b1d85c3aaa7c59a81ccb9a\n","  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=d4995ea132c13335f7df15dc7a7da28a5ce6590ab9eeca7aaffdaa383715c126\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=4d898258d3da7037ed495c4a4b295b2bb21281e651675ae5f3c29d3d13dbe1e5\n","  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=d7c26c25e693ddb5a7ade1932216c6bc1f9b59c530f78a4c089055010837d8c7\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4609 sha256=ee210c16a876cb3f22928993ceea2b375f1cd67076f58c93eb82a10cce0e65d1\n","  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n","Successfully built pinyin gdown mpld3 pycld2 word2number docopt sqlitedict langdetect pptree\n","Installing collected packages: word2number, tokenizers, sqlitedict, sentencepiece, safetensors, pycld2, pptree, pinyin, mpld3, lru-dict, janome, docopt, xxhash, terminaltables, segtok, num2words, lemminflect, langdetect, jmespath, ftfy, dill, deprecated, conllu, click, anytree, wikipedia-api, responses, OpenHowNet, multiprocess, language-tool-python, huggingface-hub, botocore, transformers, s3transfer, gdown, bpemb, datasets, boto3, accelerate, transformer-smaller-training-vocab, pytorch-revgrad, flair, bert-score, textattack\n","  Attempting uninstall: click\n","    Found existing installation: click 8.1.3\n","    Uninstalling click-8.1.3:\n","      Successfully uninstalled click-8.1.3\n","  Attempting uninstall: gdown\n","    Found existing installation: gdown 4.6.6\n","    Uninstalling gdown-4.6.6:\n","      Successfully uninstalled gdown-4.6.6\n","Successfully installed OpenHowNet-2.0 accelerate-0.20.3 anytree-2.9.0 bert-score-0.3.13 boto3-1.26.161 botocore-1.29.161 bpemb-0.3.4 click-8.0.4 conllu-4.5.3 datasets-2.4.0 deprecated-1.2.14 dill-0.3.5.1 docopt-0.6.2 flair-0.12.2 ftfy-6.1.1 gdown-4.4.0 huggingface-hub-0.15.1 janome-0.4.2 jmespath-1.0.1 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.3 lru-dict-1.2.0 mpld3-0.3 multiprocess-0.70.13 num2words-0.5.12 pinyin-0.4.0 pptree-3.1 pycld2-0.41 pytorch-revgrad-0.2.0 responses-0.18.0 s3transfer-0.6.1 safetensors-0.3.1 segtok-1.5.11 sentencepiece-0.1.99 sqlitedict-2.1.0 terminaltables-3.1.10 textattack-0.3.8 tokenizers-0.13.3 transformer-smaller-training-vocab-0.2.4 transformers-4.30.2 wikipedia-api-0.5.8 word2number-1.1 xxhash-3.2.0\n"]},{"name":"stderr","output_type":"stream","text":["textattack: Updating TextAttack package dependencies.\n","textattack: Downloading NLTK required packages.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package omw to /root/nltk_data...\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["!pip install textattack\n","from textattack.augmentation import EasyDataAugmenter"]},{"cell_type":"markdown","metadata":{"id":"yTDxAk8Jfpgr"},"source":["Import the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UeXocKZknLvp"},"outputs":[],"source":["# Link Google Drive account and download dataset\n","drive.mount('/content/gdrive')\n","copyfile('/content/gdrive/MyDrive/Magistrale/Progetto di Deep Learning/weekendUpdates.xlsx', 'weekendUpdates.xlsx')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DM029145fnTp"},"outputs":[],"source":["df = pd.read_excel(r'weekendUpdates.xlsx')\n","print(df)"]},{"cell_type":"markdown","metadata":{"id":"mj7sRARen0KD"},"source":["## Data Labelling"]},{"cell_type":"markdown","metadata":{"id":"MKKlKpv-sR_I"},"source":["We have to create a new column with the associated labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V2Wzyxtbn3PH"},"outputs":[],"source":["label = ['Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Negative', 'Neutral', 'Neutral', 'Negative', 'Neutral', 'Positive', 'Positive', 'Negative', 'Neutral', 'Positive', 'Negative', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Neutral', 'Positive', 'Negative', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Negative', 'Positive', 'Neutral', 'Positive', 'Negative', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive']\n","\n","counts = {'Positive': 0, 'Negative': 0, 'Neutral': 0}\n","\n","for el in label:\n","    counts[el] += 1\n","\n","print(f\"{counts['Positive']} positive, {counts['Negative']} negative, {counts['Neutral']} neutral\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"irDZxsxUqtF_"},"outputs":[],"source":["df.insert(2,\"label\",label)\n","df"]},{"cell_type":"markdown","metadata":{"id":"4Xv_oR9PLKQQ"},"source":["\n","\n","\n","## Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"0-E7_RV_Hhjj"},"source":["1) Data Cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I58qJug7HhT2"},"outputs":[],"source":["# Here we will erase '…', '’' and '–' because this character won't be catch in the vectorizer function\n","\n","tmp = []\n","for el in df['TextData']:\n","  string = el.replace('…','')\n","  string = string.replace('’','')\n","  tmp.append(string.replace('–',''))\n","df['TextData']=tmp\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"viW14_QPLmwG"},"outputs":[],"source":["# Here we want to traduce emoji into words, so we use the demojize function in order to extract the description of the emoticons\n","# we remove also the ':'\n","\n","df['TextData'] = df['TextData'].apply(lambda x: emoji.demojize(x).replace(':', ' '))\n","print(df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8bEuWGySgR9"},"outputs":[],"source":["from wordcloud import WordCloud\n","\n","# Generate word cloud\n","word_cloud = WordCloud(collocations=False, background_color=\"white\").generate(str(df[\"TextData\"]))\n","\n","# Display the word cloud\n","plt.imshow(word_cloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"61uGwErOz1_u"},"source":["2) Tokenization and remove the Stop Words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OekU9OjeqaHI"},"outputs":[],"source":["import string\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PhI5zfn_-OIs"},"outputs":[],"source":["# Get the stopwords list\n","stop_words = nltk.corpus.stopwords.words(\"english\")\n","\n","def preprocess_text(text):\n","    tokens = tf.strings.split(text)\n","    tokens = tf.boolean_mask(tokens, ~tf.reduce_any(tf.equal(tokens[:, tf.newaxis], list(stop_words)), axis=1))\n","    tmp = [re.sub('[^a-zA-Z0-9]', '', w.numpy().decode().lower()) for w in tokens if re.sub('[^a-zA-Z0-9]', '', w.numpy().decode().lower()) not in stop_words]\n","\n","    return tmp\n","\n","# Create an empty list to store preprocessed sentences\n","preprocessed_sentences = []\n","\n","# Apply preprocessing to each text in the dataframe and append the result to the preprocessed_sentences list\n","for el in df['TextData']:\n","    preprocessed_sentences.append(preprocess_text(el))\n","\n","# Print the preprocessed_sentences list\n","print((preprocessed_sentences))"]},{"cell_type":"markdown","metadata":{"id":"EJ4-nb2uObnR"},"source":["3) Stemming"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rmQBs2sAOhPb"},"outputs":[],"source":["# Initialize Python porter stemmer\n","ps = PorterStemmer()\n","\n","stemmed_sentences = []\n","\n","# Print original word and its stem\n","print(\"{0:20}{1:20}\".format(\"--Word--\", \"--Stem--\"))\n","\n","# Perform stemming\n","for sentence in preprocessed_sentences:\n","    stemmed_words = [ps.stem(word) for word in sentence]\n","    stemmed_sentences.append(stemmed_words)\n","\n","\n","    for word, stemmed_word in zip(sentence, stemmed_words):\n","        print(\"{0:20}{1:20}\".format(word, stemmed_word))\n","\n","preprocessed_sentences = stemmed_sentences"]},{"cell_type":"markdown","metadata":{"id":"1yjpWftSWn5C"},"source":[" 4) Lemming"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1uoaocWWmb8"},"outputs":[],"source":["# Initialize WordNet lemmatizer\n","wnl = WordNetLemmatizer()\n","\n","lemmatized_sentences = []\n","\n","# Print original word and its lemma\n","print(\"{0:20}{1:20}\".format(\"--Word--\", \"--Lemma--\"))\n","\n","# Perform lemmatization\n","for sentence in preprocessed_sentences:\n","    lemmatized_words = [wnl.lemmatize(word, pos=\"v\") for word in sentence]\n","    lemmatized_sentences.append(lemmatized_words)\n","\n","\n","    for word, lemma in zip(sentence, lemmatized_words):\n","        print(\"{0:20}{1:20}\".format(word, lemma))\n","\n","preprocessed_sentences = lemmatized_sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PreTUAdDL70R"},"outputs":[],"source":["preprocessed_sentences"]},{"cell_type":"markdown","metadata":{"id":"mNdUsxvWLXk0"},"source":["## Vectorization\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZqsUFh1346k"},"outputs":[],"source":["max_sentence = max(df['TextData'], key=len)\n","words_per_sentence = len(max_sentence.split())\n","print(f\"The max length of a sentence is {words_per_sentence}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zj4xXT3jLlSt"},"outputs":[],"source":["# Instantiate a vectorizer\n","vectorizer = TextVectorization(\n","    max_tokens=None,\n","    standardize=\"lower_and_strip_punctuation\",\n","    output_sequence_length=words_per_sentence)"]},{"cell_type":"markdown","metadata":{"id":"nWz3RAB7KPlK"},"source":["We now re-join each sentence starting from their tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w80xDdgKKPQu"},"outputs":[],"source":["sentences = [' '.join(el) for el in preprocessed_sentences]\n","labels = [df['label'][idx] for idx, _ in enumerate(preprocessed_sentences)]\n","label = labels\n","\n","sentences_def = [i.replace('’', '') for i in sentences]\n","vocabulary_def = sentences_def\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uqUOFrU2Lg4F"},"outputs":[],"source":["df_def = pd.DataFrame(list(zip(sentences_def, label)),\n","               columns =['TextData', 'label'])\n","print(df_def)"]},{"cell_type":"markdown","metadata":{"id":"0Fv4kWuYHHXv"},"source":["## Data Augmentation"]},{"cell_type":"markdown","metadata":{"id":"_NBcGgL4ZDei"},"source":["EasyDataAugmenter function:\n","\n","- pct_words_to_swap: This parameter determines the percentage of words in a sentence that will be replaced with their WordNet synonyms.\n","\n","For example, if pct_words_to_swap=0.4, 40% of the words in a sentence will be replaced. The default value is 0.1, corresponding to 10% of words.\n","\n","- transformations_per_example: It specifies the number of augmented examples generated from each input example.\n","\n","In this case, transformations_per_example=5 means that for each input sentence, the augmenter will create five augmented sentences with WordNet transformations.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxVxrpOuBN7b"},"outputs":[],"source":["new_vocabulary = []\n","wordnet_aug = EasyDataAugmenter(pct_words_to_swap=0.4,transformations_per_example=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwsJrlTeDxX-"},"outputs":[],"source":["for _ in range(30):\n","    new_sentences = [wordnet_aug.augment(i) for i in vocabulary_def]\n","    new_vocabulary.extend(new_sentences)\n","    print(*new_sentences, sep='\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VSjonirCGguT"},"outputs":[],"source":["new_vocabulary_def = []\n","new_vocabulary_def = [' '.join(el) for el in new_vocabulary] + vocabulary_def\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Cx7YdJxJG02"},"outputs":[],"source":["new_label_def = []\n","new_label_def = label * 30 + label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iw6WIPNiI3CG"},"outputs":[],"source":["df_def = pd.DataFrame(list(zip(new_vocabulary_def, new_label_def)),\n","               columns =['TextData', 'label'])\n","df_def"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p4sqlh2JixFP"},"outputs":[],"source":["label_counts = Counter(df_def['label'])\n","\n","cont_pos = label_counts['Positive']\n","cont_neg = label_counts['Negative']\n","cont_neu = label_counts['Neutral']\n","\n","print(f\"{cont_pos} positive, {cont_neg} negative, {cont_neu} neutral\")"]},{"cell_type":"markdown","metadata":{"id":"cdkIvSwTrqyF"},"source":["## Class imbalance problem\n","Undersample the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Jxm72pSati9"},"outputs":[],"source":["data_0 = df_def[df_def['label']=='Positive'].sample(n=400,random_state=16)\n","data_1 = df_def[df_def['label']=='Negative']\n","data_2 = df_def[df_def['label']=='Neutral']\n","\n","print(len(data_0))\n","print(len(data_1))\n","print(len(data_2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaom-4gtszIj"},"outputs":[],"source":["df_def = pd.concat([data_0, data_1, data_2]).sample(frac=1, random_state=42).reset_index(drop=True)\n","df_def\n"]},{"cell_type":"markdown","metadata":{"id":"e6LiCWnzLw4K"},"source":["## Dataset splitting\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmKJSdrKLwex"},"outputs":[],"source":["# Specify the ratio of training-validation data and compute the corresponding number of elements\n","split_fraction = 0.8\n","N_train = int(len(df_def) * split_fraction)\n","print(N_train)"]},{"cell_type":"markdown","metadata":{"id":"YsWTUQ0IgFxB"},"source":["For our model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuKWfKc2gFQO"},"outputs":[],"source":["Y = pd.get_dummies(df_def['label']).values #One-hot encoding: create a column for each label value\n","x_train_val_model1 = df_def[0:N_train]['TextData']\n","y_train_val_model1 = Y[0:N_train]\n","\n","x_test_model1 = df_def[N_train:]['TextData']\n","y_test_model1 = Y[N_train:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9tLXYRdigwJs"},"outputs":[],"source":["length = len(x_train_val_model1)\n","N_val = int(split_fraction * length)\n","N_val\n","\n","x_train_model1 = x_train_val_model1[0:N_val]\n","y_train_model1 = y_train_val_model1[0:N_val]\n","\n","x_val_model1 = x_train_val_model1[N_val:]\n","y_val_model1 = y_train_val_model1[N_val:]"]},{"cell_type":"markdown","metadata":{"id":"QhMyd21HgHUd"},"source":["For BERT model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gw2xoL8h1qkW"},"outputs":[],"source":["x_train_val = df_def[0:N_train]['TextData']\n","y_train_val = df_def[0:N_train]['label']\n","\n","x_test_bert = df_def[N_train:]['TextData']\n","y_test_bert = df_def[N_train:]['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRSuDpL2TiJP"},"outputs":[],"source":["length = len(x_train_val)\n","N_val = int(split_fraction * length)\n","N_val\n","\n","x_train_bert = x_train_val[0:N_val]\n","y_train_bert = y_train_val[0:N_val]\n","\n","x_val_bert = x_train_val[N_val:]\n","y_val_bert = y_train_val[N_val:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6XX9eP6ij7v"},"outputs":[],"source":["# Convert targets to pure arrays\n","y_train_model1 = np.array(y_train_model1)\n","y_val_model1 = np.array(y_val_model1)\n","y_test_model1 = np.array(y_test_model1)\n","\n","\n","y_train_bert = np.array(y_train_bert)\n","y_val_bert = np.array(y_val_bert)\n","y_test_bert = np.array(y_test_bert)"]},{"cell_type":"markdown","metadata":{"id":"4P1quW6ghhAt"},"source":["Label encoding for BERT model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sktKXCtBDg2-"},"outputs":[],"source":["import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","\n","# positive --> 2, neutral --> 1, negative --> 0\n","\n","# Initialize the LabelEncoder\n","label_encoder = LabelEncoder()\n","\n","integer_encoded = label_encoder.fit_transform(y_train_bert)\n","y_train_bert = integer_encoded\n","\n","integer_encoded = label_encoder.fit_transform(y_val_bert)\n","y_val_bert = integer_encoded\n","\n","integer_encoded = label_encoder.fit_transform(y_test_bert)\n","y_test_bert = integer_encoded"]},{"cell_type":"markdown","metadata":{"id":"qEVgYr8teLIi"},"source":["## Vocabulary creation on the Training dataset (for our model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UmBN12e3eK9i"},"outputs":[],"source":["# Vectorization of the TRAINING SENTENCES after removing stop words\n","\n","vectorizer.adapt(x_train_model1)  # apply it to the training set\n","vectorized_sentences = vectorizer(x_train_model1)   # this compute the vector matrix of the vocabulary\n","print(set(vectorizer.get_vocabulary()))  # vocabulary of the whole dataset\n","print((vectorized_sentences))  # matrix where each row is a sentence which contains the value associated to the vectorizer/vocabulary"]},{"cell_type":"markdown","metadata":{"id":"HwrOskJPLlq4"},"source":["## Word embedding (for our model)\n","\n","We decide to use an existing Word Embedding matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C74PvkpQLMWc"},"outputs":[],"source":["#import GloVe into python dictionary\n","#drive.mount('/content/gdrive')\n","#Alice Path\n","copyfile('/content/gdrive/MyDrive/glove.6B.100d.zip', 'glove.6B.100d.zip')\n","\n","# Mount Google Drive\n","#drive.mount('/content/gdrive')\n","\n","# Define the path to the GloVe file\n","#glove_path = '/content/gdrive/My Drive/Magistrale/Progetto Deep Learning/data/glove.6B.100d.zip'\n","\n","# Copy the GloVe file to the current directory\n","#copyfile(glove_path, 'glove.6B.100d.zip')\n","\n","# Extract the GloVe file\n","with zipfile.ZipFile('glove.6B.100d.zip', 'r') as zip_ref:\n","    zip_ref.extractall()\n","\n","# Load the GloVe embeddings into a dictionary\n","embeddings_index = {}\n","with open('glove.6B.100d.txt', 'r+', encoding=\"utf-8\") as f:\n","  # For each text line\n","  for line in f:\n","    # Separate the word-string from the 100-dimensional-vector-string\n","    word, coeffs = line.split(maxsplit=1)\n","    # Convert 100-dimensional vector string into a proper floating point vector\n","    coeffs = np.fromstring(coeffs, \"f\", sep=\" \")\n","    # Create a new dictionary entry\n","    embeddings_index[word] = coeffs\n","\n","print('Found %s word vectors.' % len(embeddings_index))"]},{"cell_type":"markdown","metadata":{"id":"XLzlWj4hGQ7f"},"source":["We compute now the intersection between our train vocabulary and the glove embedding"]},{"cell_type":"markdown","metadata":{"id":"RDE8BnxfdgAd"},"source":["* Size of the matrix: 20002 rows x 100 dimensions\n","(20000 words from our vocabulary + word separator + [unknown word])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZnpR3gnjkggZ"},"outputs":[],"source":["# Size of the matrix: 20002 rows x 100 dimensions\n","#   20000 words from our vocabulary + word separator + [unknown word]\n","#   30-dimensional representation from GloVe\n","total_words=0\n","for i in vectorizer.get_vocabulary():\n","    total_words += 1\n","print(total_words)\n","embedding_dim = 100  # number of GloVe dimensions (how many dimensions that capture properties of the words)\n","embedding_matrix = np.zeros((total_words+2, embedding_dim))\n","\n","# For each word in our vocabulary\n","for i, word in enumerate(vectorizer.get_vocabulary()):\n","    # Search corresponding embedding in GloVe,\n","    # and add it in the correct row of the embedding matrix\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","        print(word)\n","        print(embedding_matrix[i])\n","    # else: words not found in embedding index will be all-zeros."]},{"cell_type":"markdown","metadata":{"id":"ncGuiBlo5siF"},"source":["## Alternative Word Embedding (Word2Vec)"]},{"cell_type":"markdown","metadata":{"id":"Vyf7TITTs4MD"},"source":["This was an option for the word embedding, but because of the fact that word2vec need to be trained, we decided to use the simpler embedding model of glove. This because, after the data augmentation we have very similar sentences so a fixed coordinates can reduce the risk of overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KYmGQBqzcMj"},"outputs":[],"source":["''' from gensim.models import Word2Vec\n","from nltk.tokenize import word_tokenize\n","\n","embedding_dim = 10\n","vocabulary = [set(vectorizer.get_vocabulary())]\n","\n","# Addestramento del modello Word2Vec\n","model = Word2Vec(vocabulary, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n","\n","# Ottenere l'embedding di una parola\n","embedding = model.wv['weekend']\n","\n","# Stampa dell'embedding\n","print(\"Word Embedding:\")\n","print(embedding)  '''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_mhMgv-6MIe"},"outputs":[],"source":["''' total_words = len(vectorizer.get_vocabulary())\n","embedding_matrix = np.zeros((total_words + 2, embedding_dim))\n","\n","for i, word in enumerate(vectorizer.get_vocabulary()):\n","    embedding_matrix[i] = model.wv[word]\n","\n","    # Optional: Print word and corresponding embedding vector\n","    print(f\"Word: {word}\")\n","    print(f\"Embedding Vector: {embedding_matrix[i]}\")\n","\n","# Words not found in the embedding index will have all-zeros embeddings in the matrix\n"," '''\n"]},{"cell_type":"markdown","metadata":{"id":"q1_pNVOiLMoE"},"source":["## Model Definition and Training"]},{"cell_type":"markdown","metadata":{"id":"hKQjsvIreazJ"},"source":["1) Model from scratch (old and new)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1YW9vBVL7Y7"},"outputs":[],"source":["num_tokens = len(vectorizer.get_vocabulary())  # Size of the vocabulary\n","\n","embedding_layer = Embedding(\n","    input_dim = num_tokens,\n","    output_dim = embedding_dim,\n","    embeddings_initializer = keras.initializers.Constant(embedding_matrix),\n","    trainable = False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aa_f42e2gfr8"},"outputs":[],"source":["x_train_model1 = vectorizer(x_train_model1).numpy()\n","x_val_model1 = vectorizer(x_val_model1).numpy()\n","x_test_model1 = vectorizer(x_test_model1).numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eh-QgXqRxPbW"},"outputs":[],"source":["# OLD MODEL\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","from tensorflow.keras.metrics import SparseCategoricalAccuracy\n","\n","learning_rate = 1e-3\n","\n","model_old = Sequential()\n","model_old.add(keras.Input(shape=(words_per_sentence,), dtype='int64'))\n","model_old.add(Embedding(input_dim = num_tokens,output_dim = embedding_dim))\n","model_old.add(SpatialDropout1D(0.5))\n","model_old.add(LSTM(16, dropout=0.5, activation='relu'))\n","model_old.add(Dense(3,activation='softmax'))\n","model_old.compile(loss = 'categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=learning_rate),metrics = ['Accuracy'])\n","print(model_old.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tYkfYBzoW5rg"},"outputs":[],"source":["# NEW MODEL\n","\n","import random\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","\n","learning_rate = 1e-3\n","num_classes = 3\n","\n","model_new = keras.Sequential([\n","    keras.Input(shape=(words_per_sentence,), dtype='int64'),\n","    Embedding(input_dim=num_tokens, output_dim=embedding_dim),\n","    LSTM(16, dropout=0.2, activation='tanh'),\n","    Dense(num_classes, activation='softmax')\n","])\n","\n","optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","model_new.compile(\n","    loss='categorical_crossentropy',\n","    optimizer=optimizer,\n","    metrics=['Accuracy']\n",")\n","\n","print(model_new.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCjVS_bfCMEZ"},"outputs":[],"source":["history_old = model_old.fit(x_train_model1,\n","          y_train_model1,\n","          epochs = 25,\n","          batch_size=64,\n","          validation_data=(x_val_model1, y_val_model1),\n","          verbose = 1);\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWXoBBxfbrG_"},"outputs":[],"source":["history_new = model_new.fit(x_train_model1,\n","          y_train_model1,\n","          epochs = 25,\n","          batch_size=64,\n","          validation_data=(x_val_model1, y_val_model1),\n","          verbose = 1);\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jsdME8zJb5xc"},"outputs":[],"source":["batch_size = 64\n","score, acc = model_old.evaluate(x_test_model1, y_test_model1, verbose=2, batch_size=batch_size)\n","print(f\"score: {score:.2f}\")\n","print(f\"acc: {acc:.2f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wnYVnJIFCO6_"},"outputs":[],"source":["batch_size = 64\n","score, acc = model_new.evaluate(x_test_model1, y_test_model1, verbose=2, batch_size=batch_size)\n","print(f\"score: {score:.2f}\")\n","print(f\"acc: {acc:.2f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"tnZLHbwEoJpd"},"source":["## Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O0E7QkfLL717"},"outputs":[],"source":["## Plot accuracy changes from model.fit()\n","from matplotlib import pyplot as plt\n","\n","plt.figure()\n","plt.plot(history_old.history['Accuracy'], label='Train Accuracy')\n","plt.plot(history_old.history['val_Accuracy'], label='Validation Accuracy')\n","plt.plot(history_old.history['loss'], label='Train Loss')\n","plt.plot(history_old.history['val_loss'], label='Validation Loss')\n","plt.title('Model from scratch (old)')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy & Loss')\n","plt.legend(loc='upper left')\n","\n","\n","plt.show()\n","plt.savefig('/content/gdrive/MyDrive/Progetto Deep Learning/data/metrics_old.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrRv72ePABUO"},"outputs":[],"source":["plt.figure()\n","plt.plot(history_new.history['Accuracy'], label='Train Accuracy')\n","plt.plot(history_new.history['val_Accuracy'], label='Validation Accuracy')\n","plt.plot(history_new.history['loss'], label='Train Loss')\n","plt.plot(history_new.history['val_loss'], label='Validation Loss')\n","plt.title('Model from scratch (new)')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy & Loss')\n","plt.legend(loc='upper left')\n","plt.show()\n","\n","plt.savefig('/content/gdrive/MyDrive/Progetto Deep Learning/data/metrics_old.png')"]},{"cell_type":"markdown","metadata":{"id":"5jLz9GuK_yRp"},"source":["2) Model BERT pre-trained"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPJgS3EgQxZP"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","from transformers import BertTokenizer, TFBertForSequenceClassification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eeB0jxoruQc8"},"outputs":[],"source":["num_classes = 3\n","\n","bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a_1KTXAvubzj"},"outputs":[],"source":["y_train_bert = np.array(y_train_bert)\n","y_test_bert = np.array(y_test_bert)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59gdQNbsulVP"},"outputs":[],"source":["def convert_example_to_feature(review):\n","  return bert_tokenizer.encode_plus(review,\n","                add_special_tokens = True,     # add [CLS], [SEP]\n","                max_length = 512,\n","                padding='max_length',\n","                truncation=True,\n","                return_attention_mask = True,  # add attention mask to not focus on pad tokens\n","              )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8XgMU7xuoPw"},"outputs":[],"source":["def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n","  return {\n","      \"input_ids\": input_ids,\n","      \"token_type_ids\": token_type_ids,\n","      \"attention_mask\": attention_masks,\n","  }, label\n","\n","def encode_examples(ds):\n","  # prepare list, so that we can build up final TensorFlow dataset from slices.\n","  input_ids_list = []\n","  token_type_ids_list = []\n","  attention_mask_list = []\n","  label_list = []\n","  for review, label in ds:\n","    bert_input = convert_example_to_feature(review)\n","    input_ids_list.append(bert_input['input_ids'])\n","    token_type_ids_list.append(bert_input['token_type_ids'])\n","    attention_mask_list.append(bert_input['attention_mask'])\n","    label_list.append([label])\n","\n","  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NnU5lZ-tuqnU"},"outputs":[],"source":["# hyper-parameters\n","batch_size = 8\n","# train dataset\n","ds_train = zip(x_train_bert, y_train_bert)\n","ds_test = zip(x_test_bert, y_test_bert)\n","ds_train_encoded = encode_examples(ds_train).batch(batch_size)\n","ds_test_encoded = encode_examples(ds_test).batch(batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQ69h_lsvBfb"},"outputs":[],"source":["model_save_path = './sentiment-analysis-using-bert-keras/models/bert_model.h5'\n","\n","path = \"./sentiment-analysis-using-bert-keras/models/\"\n","\n","## Initialize pre-built BERT-based classifier from transformers\n","bert_model = TFBertForSequenceClassification.from_pretrained(\n","    'bert-base-uncased', num_labels=num_classes)\n","\n","bert_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nMsUblSKvFdb"},"outputs":[],"source":["# multiple epochs might be better as long as we will not overfit the model\n","number_of_epochs = 5\n","learning_rate = 1e-5\n","\n","# choosing Adam optimizer\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n","# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","bert_model.compile(loss=loss,\n","                   optimizer=optimizer,\n","                   metrics=metric)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4kqP_348vIMo"},"outputs":[],"source":["history = bert_model.fit(ds_train_encoded,\n","                         batch_size=batch_size,\n","                         epochs=number_of_epochs,\n","                         validation_data=ds_test_encoded)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJInHUB-OzEV"},"outputs":[],"source":["batch_size = 8\n","score, acc = bert_model.evaluate(ds_test_encoded, batch_size=batch_size)\n","print(f\"score: {score:.2f}\")\n","print(f\"acc: {acc:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PpzZCMs6xpVv"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib\n","import pandas as pd\n","\n","matplotlib.rcParams['figure.dpi'] = 150\n","\n","\n","# Plotting results\n","def plot1(history):\n","    acc = history.history['accuracy']\n","    val_acc = history.history['val_accuracy']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","\n","    epochs = range(1, len(acc) + 1)\n","    ## Accuracy plot\n","    plt.plot(epochs, acc, 'bo', label='Training acc')\n","    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","    plt.title('Training and validation accuracy')\n","    plt.legend()\n","    ## Loss plot\n","    plt.figure()\n","\n","    plt.plot(epochs, loss, 'bo', label='Training loss')\n","    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.legend()\n","    plt.show()\n","\n","\n","def plot2(history):\n","    pd.DataFrame(history.history).plot(figsize=(8, 5))\n","    plt.grid(True)\n","    #plt.gca().set_ylim(0,1)\n","    plt.show()\n","plot2(history)"]},{"cell_type":"markdown","metadata":{"id":"EM3Kb_Er1pXp"},"source":["##Model Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ge-vlQ8L46pC"},"outputs":[],"source":["def plot_confusion_matrix(cm,\n","                          target_names,\n","                          title='Confusion matrix',\n","                          cmap=None,\n","                          normalize=True):\n","    import matplotlib.pyplot as plt\n","    import numpy as np\n","    import itertools\n","\n","    accuracy = np.trace(cm) / float(np.sum(cm))\n","    misclass = 1 - accuracy\n","\n","    if cmap is None:\n","        cmap = plt.get_cmap('Blues')\n","\n","    plt.figure(figsize=(8, 6), dpi=150)\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","\n","    if target_names is not None:\n","        tick_marks = np.arange(len(target_names))\n","        plt.xticks(tick_marks, target_names, rotation=45)\n","        plt.yticks(tick_marks, target_names)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        if normalize:\n","            plt.text(j,\n","                     i,\n","                     \"{:0.4f}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","        else:\n","            plt.text(j,\n","                     i,\n","                     \"{:,}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(\n","        accuracy, misclass))\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHLhelHft-EF"},"outputs":[],"source":["# old\n","y_test_pred_mod1 =model_old.predict(x_test_model1)\n","y_test_pred_class_mod1 = np.argmax(y_test_pred_mod1,axis=1)\n","reverse_encoded = np.argmax(y_test_model1, axis=1)\n","\n","cm_old = confusion_matrix(list(reverse_encoded), list(y_test_pred_class_mod1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLrXtdCU37od"},"outputs":[],"source":["plot_confusion_matrix(cm_old,\n","                      normalize=False,\n","                      target_names=['neutral', 'negative', 'positive'],\n","                      title=\"Confusion Matrix old model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6QOv27tH34Tj"},"outputs":[],"source":["# new\n","y_test_pred_mod1 =model_new.predict(x_test_model1)\n","y_test_pred_class_mod1 = np.argmax(y_test_pred_mod1,axis=1)\n","reverse_encoded = np.argmax(y_test_model1, axis=1)\n","\n","cm_new= confusion_matrix(list(reverse_encoded),list(y_test_pred_class_mod1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JwQ9STGqyKBE"},"outputs":[],"source":["plot_confusion_matrix(cm_new,\n","                      normalize=False,\n","                      target_names=['neutral', 'negative', 'positive'],\n","                      title=\"Confusion Matrix new model \")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NHikRxKNwWWV"},"outputs":[],"source":["# bert\n","y_test_pred_bert = bert_model.predict(ds_test_encoded, batch_size=batch_size)\n","y_test_pred_class_bert = y_test_pred_bert[0].argmax(axis=1)\n","cm_bert = confusion_matrix(list(y_test_bert),list(y_test_pred_class_bert))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MnrNbbrOTal"},"outputs":[],"source":["plot_confusion_matrix(cm_bert,\n","                      normalize=False,\n","                      target_names=['neutral', 'negative', 'positive'],\n","                      title=\"Confusion Matrix BERT model \")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1z91j2-eysuf"},"outputs":[],"source":["save_path = '/content/gdrive/MyDrive/Progetto Deep Learning/model_from_scratch_old'  # Update with your desired path and model name\n","model_old.save(save_path)\n","save_path = '/content/gdrive/MyDrive/Progetto Deep Learning/bert_model'  # Update with your desired path and model name\n","bert_model.save(save_path)\n","\n","save_path = '/content/gdrive/MyDrive/Progetto Deep Learning/model_from_scratch_new' # Update with your desired path and model name\n","model_new.save(save_path)"]},{"cell_type":"markdown","metadata":{"id":"gJJJtg1Dyd2x"},"source":["## Test with another set of sentences"]},{"cell_type":"markdown","metadata":{"id":"purqUWWA6Tag"},"source":["\n","Positive Reviews:\n","1. \"Ready to explore the beautiful beaches of Bali! 🏖️🌴 #vacationmode\" - Label: Positive\n","\n","2. \"The trip was an absolute blast! Unforgettable memories were made. 🌟😀🌴 #amazing #vacation\" - Label: Positive\n","\n","Negative Reviews:\n","1. \" Wishing the vacation could last forever. 😢 #vacation\" - Label: Negative\n","\n","2. \"In desperate need of a relaxing getaway. 😫 #vacationdreams\" - Label: Negative\n","\n","Neutral Reviews:\n","1. \"Saturday night calls for a cozy pizza night in! But in desperate need of a relaxing getaway.\" - Label: Neutral\n","\n","2. \" BBQ party at the lakeside, time to indulge in delicious food!  However it rained!\" - Label: Neutral\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgODzXv76XDn"},"outputs":[],"source":["df_new_sentences=[\"Ready to explore the beautiful beaches of Bali! 🏖️🌴 #vacation\", \"Saturday night calls for a cozy pizza night in! But in desperate need of a relaxing getaway.\", \" BBQ party at the lakeside, time to indulge in delicious food! However it rained!\", \" Wishing the vacation could last forever. 😢 #vacation\", \"In desperate need of a relaxing getaway. 😫 #vacationdreams\" ,  \"The trip was an absolute blast! Unforgettable memories were made. 🌟😀🌴 #amazing #vacation\"]\n","df_label=[\"Positive\", \"Neutral\", \"Neutral\", \"Negative\", \"Negative\",\"Positive\"]\n","df_new=pd.DataFrame({\"sentences\":df_new_sentences, \"label\":df_label})\n","df_new"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZIg30_fXTRgx"},"outputs":[],"source":["integer_encoded = label_encoder.fit_transform(df_label)\n","df_label = integer_encoded"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W0W40CX-1w-O"},"outputs":[],"source":["#bert\n","test_new = encode_examples(zip(pd.Series(df_new_sentences),df_label)).batch(batch_size)\n","\n","test_new_bert = bert_model.predict(test_new, batch_size=batch_size)\n","test_new_class_bert = test_new_bert[0].argmax(axis=1)\n","cm = confusion_matrix(list(df_label),list(test_new_class_bert))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmsLYi815ztJ"},"outputs":[],"source":["plot_confusion_matrix(cm,\n","                      normalize=False,\n","                      target_names=['neutral', 'negative', 'positive'],\n","                      title=\"Confusion Matrix bert model on new data\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBoSB13fq6yf"},"outputs":[],"source":["#old model\n","test_new_mod1 = model_old.predict(vectorizer(df_new_sentences))\n","y_test_pred_class_mod1 = np.argmax(test_new_mod1,axis=1)\n","cm_old_= confusion_matrix(list(df_label),list(y_test_pred_class_mod1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZRlGejRvX20"},"outputs":[],"source":["plot_confusion_matrix(cm_old_,\n","                      normalize=False,\n","                      target_names=['neutral', 'negative', 'positive'],\n","                      title=\"Confusion Matrix old model on new data\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTTLl52_jWgH"},"outputs":[],"source":["#new model\n","test_new_mod2 = model_new.predict(vectorizer(df_new_sentences))\n","y_test_pred_class_mod2 = np.argmax(test_new_mod2,axis=1)\n","cm_new_= confusion_matrix(list(df_label),list(y_test_pred_class_mod2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XLRgM8SMjdVS"},"outputs":[],"source":["plot_confusion_matrix(cm_new_,\n","                      normalize=False,\n","                      target_names=['neutral', 'negative', 'positive'],\n","                      title=\"Confusion Matrix new model on new data\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}